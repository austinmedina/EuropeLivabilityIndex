{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is only meant to be run once. Every file contained a header and an explaination of the file that I needed to remove before reading the file as a csv. So this code goes into each file, reads it, and prints back lines 20 onward, therefore removing the header. It then reads the file as a csv, into a dataframe. It does this for everyfile and concatonates all of the dataframes together so they can easily be querried. Below is an example of the header:\n",
    "\n",
    "EUROPEAN CLIMATE ASSESSMENT & DATASET (ECA&D), file created on: 25-11-2023\n",
    "THESE DATA CAN BE USED FOR NON-COMMERCIAL RESEARCH AND EDUCATION PROVIDED THAT THE FOLLOWING SOURCE IS ACKNOWLEDGED: \n",
    "\n",
    "Klein Tank, A.M.G. and Coauthors, 2002. Daily dataset of 20th-century surface\n",
    "air temperature and precipitation series for the European Climate Assessment.\n",
    "Int. J. of Climatol., 22, 1441-1453.\n",
    "Data and metadata available at http://www.ecad.eu\n",
    "\n",
    "FILE FORMAT (MISSING VALUE CODE = -9999):\n",
    "\n",
    "01-06 STAID: Station identifier\n",
    "08-13 SOUID: Source identifier\n",
    "15-22 DATE : Date YYYYMMDD\n",
    "24-28 TG   : Mean temperature in 0.1 &#176;C\n",
    "30-34 Q_TG : quality code for TG (0='valid'; 1='suspect'; 9='missing')\n",
    "\n",
    "This is the blended series of station VAEXJOE, SWEDEN (STAID: 1)\n",
    "Blended and updated with sources:35381 35405 \n",
    "See files sources.txt and stations.txt for more info.\n",
    "\n",
    "STAID, SOUID,    DATE,   TG, Q_TG\n",
    "     1, 35381,18600101,   21,    1\n",
    "     1, 35381,18600102,   46,    1\n",
    "     1, 35381,18600103,   31,    1\n",
    "....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWeatherData(readDirectory, csvFile, dfCSVFile):\n",
    "    \n",
    "    directory = os.fsencode(readDirectory)\n",
    "    columns = ''\n",
    "    pdCol = []\n",
    "    multiplier = 0\n",
    "\n",
    "    filenameByte = os.path.join(directory, os.listdir(directory)[0])\n",
    "    filename = filenameByte.decode('utf-8')\n",
    "\n",
    "    with open(filename, \"r\") as t:\n",
    "        lines = t.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'TG,' in line:\n",
    "                columns = 'STAID, SOUID,    DATE,   TG, Q_TG\\n'\n",
    "                pdCol = ['STAID','SOUID','DATE','TG','Q_TG']\n",
    "                multiplier = 0.1\n",
    "                break\n",
    "            elif 'SD' in line:\n",
    "                columns = 'STAID, SOUID,    DATE,   SD, Q_SD\\n'\n",
    "                pdCol = ['STAID','SOUID','DATE','SD','Q_SD']\n",
    "                multiplier = 1\n",
    "                break\n",
    "            elif 'RR' in line:\n",
    "                columns = 'STAID, SOUID,    DATE,   RR, Q_RR\\n'\n",
    "                pdCol = ['STAID','SOUID','DATE','RR','Q_RR']\n",
    "                multiplier = .1\n",
    "                break\n",
    "\n",
    "    with open(csvFile, \"w\") as c:\n",
    "        c.write(columns)\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            filenameByte = os.path.join(directory, file)\n",
    "\n",
    "            if os.path.isfile(filenameByte):\n",
    "                filename = filenameByte.decode('utf-8')\n",
    "                with open(filename, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    lineNumber = 0\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        if ',20' in line:\n",
    "                            break\n",
    "                        lineNumber = lineNumber + 1\n",
    "                    \n",
    "                    for line in lines[lineNumber:]:\n",
    "                        c.write(line)\n",
    "\n",
    "    weatherData = pd.read_csv(csvFile)\n",
    "    weatherData.columns = pdCol\n",
    "    weatherDataFilter = weatherData[weatherData[pdCol[4]] != 9]\n",
    "    weatherDataFilter = weatherDataFilter[weatherDataFilter[pdCol[4]] != 1]\n",
    "    weatherDataFilter['DATE'] = pd.to_datetime(weatherDataFilter['DATE'], format='%Y%m%d')\n",
    "    weatherDataFilter = weatherDataFilter.drop(columns=['SOUID', pdCol[4]])\n",
    "    weatherDataFilter[pdCol[3]] = weatherDataFilter[pdCol[3]] * multiplier\n",
    "\n",
    "    weatherDataFilter.to_csv(dfCSVFile, index=False)\n",
    "\n",
    "    return weatherDataFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStationData():\n",
    "    with open(\"../Data/Weather/AverageDailyTemp/sources.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(\"../Data/Weather/AverageDailyTemp/sources.txt\", \"w\") as f:\n",
    "        lineNumber = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'STAID, SOUID' in line:\n",
    "                break\n",
    "            lineNumber = lineNumber + 1\n",
    "        \n",
    "        for line in lines[lineNumber:]:\n",
    "            f.write(line)\n",
    "    \n",
    "    stationData = pd.read_csv(\"../Data/Weather/AverageDailyTemp/sources.txt\", on_bad_lines='warn')\n",
    "\n",
    "    return stationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationData = readStationData()\n",
    "stationData.columns = ['STAID','SOUID','SOUNAME','CN','LAT','LON','HGHT','ELEI','START','STOP','PARID','PARNAME']\n",
    "stationDataFiltered = stationData.drop(columns=['SOUNAME', 'HGHT', 'ELEI', 'PARID', 'PARNAME', 'START', 'STOP'])\n",
    "stationDataAPI = stationDataFiltered[stationDataFiltered['LAT'] != '         ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocation(lati, long):\n",
    "    API_KEY = 'AIzaSyCt4xS1ED90qZjG2MpmznJu0ENQ2hl3z9Y'\n",
    "\n",
    "    lat = lati.split(':')\n",
    "    latitude = float(lat[0]) + (int(lat[1])/60) + (int(lat[2])/3600)\n",
    "    lng = long.split(':')\n",
    "    longitude = float(lng[0]) + (int(lng[1])/60) + (int(lng[2])/3600)\n",
    "\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?latlng=' + str(latitude) + ',' + str(longitude) + '&result_type=administrative_area_level_1|country&key=' + API_KEY\n",
    "    googleResponse = requests.post(url)\n",
    "    gr = googleResponse.json()\n",
    "\n",
    "    city = ''\n",
    "    country = ''\n",
    "\n",
    "    if (gr['status'] == 'OK'):\n",
    "        for e in gr['results'][0]['address_components']:\n",
    "            for type in e['types']:\n",
    "                if (type == 'administrative_area_level_1'):\n",
    "                    city = e['long_name']\n",
    "                    break\n",
    "                elif (type == 'country'):\n",
    "                    country = e['long_name']\n",
    "                    break\n",
    "                \n",
    "    return  city + ', ' + country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\austi\\AppData\\Local\\Temp\\ipykernel_2580\\1642991849.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stationDataAPI['LOCATION'] = stationDataAPI.apply(lambda x: getLocation(x['LAT'], x['LON']), axis=1)\n",
      "C:\\Users\\austi\\AppData\\Local\\Temp\\ipykernel_2580\\1642991849.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stationDataResults[['MUNICIPALITY', 'COUNTRY', 'ToBeDropped']] = stationDataResults['LOCATION'].str.split(',', expand=True)\n",
      "C:\\Users\\austi\\AppData\\Local\\Temp\\ipykernel_2580\\1642991849.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stationDataResults[['MUNICIPALITY', 'COUNTRY', 'ToBeDropped']] = stationDataResults['LOCATION'].str.split(',', expand=True)\n",
      "C:\\Users\\austi\\AppData\\Local\\Temp\\ipykernel_2580\\1642991849.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stationDataResults[['MUNICIPALITY', 'COUNTRY', 'ToBeDropped']] = stationDataResults['LOCATION'].str.split(',', expand=True)\n"
     ]
    }
   ],
   "source": [
    "stationDataAPI['LOCATION'] = stationDataAPI.apply(lambda x: getLocation(x['LAT'], x['LON']), axis=1)\n",
    "stationDataResults = stationDataAPI\n",
    "stationDataResults[['MUNICIPALITY', 'COUNTRY', 'ToBeDropped']] = stationDataResults['LOCATION'].str.split(',', expand=True)\n",
    "stationDataResults = stationDataResults.drop(columns = ['SOUID', 'LAT', 'LON', 'LOCATION', 'ToBeDropped'])\n",
    "stationDataResultsTrimmed = stationDataResults[stationDataResults['MUNICIPALITY'] != '']\n",
    "stationDataResultsTrimmed = stationDataResultsTrimmed.drop_duplicates('STAID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationDataResultsTrimmed.to_csv(\"../Data/Weather/locations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationLocations = pd.read_csv(\"../Data/Weather/locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperaturePath = \"../Data/Weather/AverageDailyTemp/RawAvgTempData\"\n",
    "temperatureCSV = \"../Data/Weather/AverageDailyTemp/AverageDailyTemp.csv\"\n",
    "temperatureFilteredCSV = \"../Data/Weather/AverageDailyTemp/DailyTempFiltered.csv\"\n",
    "snowPath = \"../Data/Weather/AverageDailySnowfall/RawSnowData\"\n",
    "snowCSV = \"../Data/Weather/AverageDailySnowfall/AverageDailySnow.csv\"\n",
    "snowFilteredCSV = \"../Data/Weather/AverageDailySnowfall/DailySnowFiltered.csv\"\n",
    "rainPath = \"../Data/Weather/AverageDailyRain/RawRainData\"\n",
    "rainCSV = \"../Data/Weather/AverageDailyRain/AverageDailyRain.csv\"\n",
    "rainFilteredCSV = \"../Data/Weather/AverageDailyRain/DailyRainFiltered.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempData = readWeatherData(temperaturePath, temperatureCSV, temperatureFilteredCSV)\n",
    "snowData = readWeatherData(snowPath, snowCSV, snowFilteredCSV)\n",
    "rainData = readWeatherData(rainPath, rainCSV, rainFilteredCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempData = pd.read_csv(temperatureFilteredCSV)\n",
    "snowData = pd.read_csv(snowFilteredCSV)\n",
    "rainData = pd.read_csv(rainFilteredCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationDataResultsTrimmed = stationLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempLocation = tempData.merge(stationDataResultsTrimmed, left_on = 'STAID', right_on = 'STAID', how = 'left')\n",
    "snowLocation = snowData.merge(stationDataResultsTrimmed, left_on = 'STAID', right_on = 'STAID', how = 'left')\n",
    "rainLocation = rainData.merge(stationDataResultsTrimmed, left_on = 'STAID', right_on = 'STAID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempLocation['DATE'] = pd.to_datetime(tempLocation['DATE'], format='%Y-%m-%d')\n",
    "snowLocation['DATE'] = pd.to_datetime(snowLocation['DATE'], format='%Y-%m-%d')\n",
    "rainLocation['DATE'] = pd.to_datetime(rainLocation['DATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season(x):\n",
    "    spring = range(60, 152)\n",
    "    summer = range(152, 244)\n",
    "    fall = range(244, 355)\n",
    "    \n",
    "    if x in spring:\n",
    "        return 'Spring'\n",
    "    if x in summer:\n",
    "        return 'Summer'\n",
    "    if x in fall:\n",
    "        return 'Fall'\n",
    "    else :\n",
    "        return 'Winter'\n",
    "    \n",
    "def groupSeasons(data):    \n",
    "    data['SEASON'] = data['DATE'].dt.dayofyear.apply(lambda x : season(x))\n",
    "    data = data.drop(columns='DATE')\n",
    "    if ('TG' in data.columns):\n",
    "        locationAverageData = data.groupby(['SEASON', 'MUNICIPALITY', 'COUNTRY']).agg({'TG':['mean']})\n",
    "        locationAverageData.columns = ['Average Temp (C)']\n",
    "    elif ('SD' in data.columns):\n",
    "        locationAverageData = data.groupby(['SEASON', 'MUNICIPALITY', 'COUNTRY']).agg({'SD':['mean']})\n",
    "        locationAverageData.columns = ['Average Snow Depth (cm)']\n",
    "    elif ('RR' in data.columns):\n",
    "        locationAverageData = data.groupby(['SEASON', 'MUNICIPALITY', 'COUNTRY']).agg({'RR':['mean']})\n",
    "        locationAverageData.columns = ['Average Rainfall (cm)']\n",
    "    \n",
    "    return locationAverageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempByLocSeasons = groupSeasons(tempLocation)\n",
    "snowByLocSeasons = groupSeasons(snowLocation)\n",
    "rainByLocSeasons = groupSeasons(rainLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData = tempByLocSeasons.merge(snowByLocSeasons, left_on = ['SEASON', 'MUNICIPALITY', 'COUNTRY'], right_on = ['SEASON', 'MUNICIPALITY', 'COUNTRY'], how = 'left')\n",
    "weatherData = weatherData.merge(rainByLocSeasons, left_on = ['SEASON', 'MUNICIPALITY', 'COUNTRY'], right_on = ['SEASON', 'MUNICIPALITY', 'COUNTRY'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData = weatherData.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData = weatherData.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.to_csv(\"../Data/Weather/weatherData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Average Temp (C)', 'Average Snow Depth (cm)', 'Average Rainfall (cm)'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherData.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
